{
  "cells": [
    {
      "cell_type": "raw",
      "id": "63ee3f93",
      "metadata": {
        "id": "63ee3f93"
      },
      "source": [
        "---\n",
        "sidebar_position: 0\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain[google-vertexai]\"\n",
        "!pip install langchain-core langgraph>0.2.27\n",
        "\n"
      ],
      "metadata": {
        "id": "FQqnsoiwUHiz"
      },
      "id": "FQqnsoiwUHiz",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4Kv_tnyaU5yQ"
      },
      "id": "4Kv_tnyaU5yQ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"automatation-trading\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "STAGING_BUCKET = \"gs://llm-mariano\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
      ],
      "metadata": {
        "id": "fggMV6WVVVsH"
      },
      "id": "fggMV6WVVVsH",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e4b41234",
      "metadata": {
        "id": "e4b41234"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"gemini-2.0-flash-001\", model_provider=\"google_vertexai\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1b2481f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b2481f0",
        "outputId": "2d063644-0d09-412b-dd66-375ea795593c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ciao!\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 9, 'candidates_token_count': 3, 'total_token_count': 12, 'prompt_tokens_details': [{'modality': 1, 'token_count': 9}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 3}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.012654953946669897}, id='run-800cbd28-af88-4807-bfdc-343a2a6994b2-0', usage_metadata={'input_tokens': 9, 'output_tokens': 3, 'total_tokens': 12})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(\"hi!\"),\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0abb0863-bee7-448d-b013-79d8db01e330",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0abb0863-bee7-448d-b013-79d8db01e330",
        "outputId": "97a8df09-277b-4d51-ddf1-c3da642bb2e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao|!\n",
            "|"
          ]
        }
      ],
      "source": [
        "for token in model.stream(messages):\n",
        "    print(token.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3e73cc20",
      "metadata": {
        "id": "3e73cc20"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f781b3cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f781b3cb",
        "outputId": "497a466a-f722-42ad-ea77-2ee270586ea4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2159b619",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2159b619",
        "outputId": "d5c0144a-46d0-4bba-c5d4-b91880a38219"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "prompt.to_messages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3a509d8c-e122-4641-b9ee-91bc23aa155a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a509d8c-e122-4641-b9ee-91bc23aa155a",
        "outputId": "b7a3f953-2cce-4476-ab1e-293436c1f34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke([HumanMessage(\"What is my name?\")])"
      ],
      "metadata": {
        "id": "QMohw-mupH5K",
        "outputId": "6dc6b350-0231-4d45-dc1c-c1d622630ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QMohw-mupH5K",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I am an AI and do not know your name.\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 5, 'candidates_token_count': 12, 'total_token_count': 17, 'prompt_tokens_details': [{'modality': 1, 'token_count': 5}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 12}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.3538728952407837}, id='run-bc70d341-3602-400a-b14b-7cad4b792e69-0', usage_metadata={'input_tokens': 5, 'output_tokens': 12, 'total_tokens': 17})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
        "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
        "        HumanMessage(content=\"What's my name?\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "wuiaUmUhrXuy",
        "outputId": "0dc9ed1b-df66-44b7-d2a0-694286b3c249",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wuiaUmUhrXuy",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Bob. You just told me! ðŸ˜‰\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 22, 'candidates_token_count': 12, 'total_token_count': 34, 'prompt_tokens_details': [{'modality': 1, 'token_count': 22}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 12}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.2570093274116516}, id='run-49a25575-7c80-483b-9d46-4f3e4eabb0bc-0', usage_metadata={'input_tokens': 22, 'output_tokens': 12, 'total_tokens': 34})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint"
      ],
      "metadata": {
        "id": "kkcHbTdbsHWp"
      },
      "id": "kkcHbTdbsHWp"
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "\n",
        "class State(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "  language: str\n",
        "\n",
        "\n",
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You talk like a pirate. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def call_model(state: State):\n",
        "    prompt = prompt_template.invoke(state)\n",
        "    response = model.invoke(prompt)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "workflow.add_edge(START,\"model\")\n",
        "workflow.add_node(\"model\",call_model)\n",
        "\n",
        "memory_saver = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory_saver)\n"
      ],
      "metadata": {
        "id": "7rROYRmhroTJ"
      },
      "id": "7rROYRmhroTJ",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"we\"}}"
      ],
      "metadata": {
        "id": "Z_PgDOkdumO8"
      },
      "id": "Z_PgDOkdumO8",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hola Soy  Bob\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages, \"language\":\"Spanish\"}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "E3EYtm3Fuumi",
        "outputId": "ce16e6da-0e9b-4400-8efe-0eaee554745f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "E3EYtm3Fuumi",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Â¡Ah, con que \"Bob\" es tu nombre! Â¡Un placer conocerte, marinero Bob! Â¿QuÃ© te trae a mi bordo? Â¿Tienes alguna pregunta para este viejo lobo de mar?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "JLx0E67QvGkF",
        "outputId": "5eaf9335-0fd9-4fa9-cf84-9a62c7fb20fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JLx0E67QvGkF",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Â¡Jajaja! Â¡QuÃ© despistado estÃ¡s, Bob! Â¡Acabas de decirme que te llamas Bob! Â¡Soy un viejo lobo de mar, no un loro! Â¿Necesitas un trago de grog para refrescar la memoria?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cJ6iOVkqvbHW"
      },
      "id": "cJ6iOVkqvbHW",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning engine\n",
        "The Reasoning Engine API provides the managed runtime for your customized agentic workflows in generative AI applications. You can create an application using orchestration frameworks such as LangChain, and deploy it with Reasoning Engine. This service has all the security, privacy, observability, and scalability benefits of Vertex AI integration."
      ],
      "metadata": {
        "id": "ykB-xAbeZBAX"
      },
      "id": "ykB-xAbeZBAX"
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview import reasoning_engines\n"
      ],
      "metadata": {
        "id": "l6CtfRPzYUju"
      },
      "id": "l6CtfRPzYUju",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_exchange_rate(\n",
        "    currency_from: str = \"USD\",\n",
        "    currency_to: str = \"EUR\",\n",
        "    currency_date: str = \"latest\",\n",
        "):\n",
        "    \"\"\"Retrieves the exchange rate between two currencies on a specified date.\"\"\"\n",
        "    import requests\n",
        "\n",
        "    response = requests.get(\n",
        "        f\"https://api.frankfurter.app/{currency_date}\",\n",
        "        params={\"from\": currency_from, \"to\": currency_to},\n",
        "    )\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "24gR77klZBwW"
      },
      "id": "24gR77klZBwW",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = reasoning_engines.LangchainAgent(\n",
        "    model= \"gemini-1.5-pro\",\n",
        "    tools=[get_exchange_rate],\n",
        "    agent_executor_kwargs={\"return_intermediate_steps\": True},\n",
        ")"
      ],
      "metadata": {
        "id": "fOq-p9rbZEG_"
      },
      "id": "fOq-p9rbZEG_",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(input=\"What's the exchange rate from US dollars to Euros currency today?\")\n"
      ],
      "metadata": {
        "id": "D6GytEgeZGoI"
      },
      "id": "D6GytEgeZGoI",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response['output']"
      ],
      "metadata": {
        "id": "HU_gXuFiZIjO",
        "outputId": "764171e3-4ea6-493c-c914-ea8bfe7a6fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "id": "HU_gXuFiZIjO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The exchange rate from US dollars to Euros is 0.96367. That means for every 1 US dollar, you would get 0.96367 Euros.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remote_agent = reasoning_engines.ReasoningEngine.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,reasoningengine]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic>=2.10\",\n",
        "        \"requests\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "LSqJDbcqZ49C",
        "outputId": "ac853998-2aa1-4410-feb9-90a41b4d29df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "id": "LSqJDbcqZ49C",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vertexai.reasoning_engines._reasoning_engines:Using bucket llm-mariano\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Writing to gs://llm-mariano/reasoning_engine/reasoning_engine.pkl\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Writing to gs://llm-mariano/reasoning_engine/requirements.txt\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Creating in-memory tarfile of extra_packages\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Writing to gs://llm-mariano/reasoning_engine/dependencies.tar.gz\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Creating ReasoningEngine\n",
            "INFO:vertexai.reasoning_engines._reasoning_engines:Create ReasoningEngine backing LRO: projects/571915015654/locations/us-central1/reasoningEngines/934057118028267520/operations/3401376500841185280\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InternalServerError",
          "evalue": "500 \n Please navigate to the Cloud Console Log Explorer page (https://console.cloud.google.com/logs/query;query=resource.type=\"aiplatform.googleapis.com%2FReasoningEngine\"%0Aresource.labels.reasoning_engine_id=~\"934057118028267520\"?project=571915015654) to view the specific errors. Additionally, please check our troubleshooting guide (https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/troubleshooting/deploy) for more information, or visit our Cloud community forum (https://www.googlecloudcommunity.com/gc/AI-ML/bd-p/cloud-ai-ml) or GitHub repository (https://github.com/googleapis/python-aiplatform/issues) to find answers and ask for help. 13: \n Please navigate to the Cloud Console Log Explorer page (https://console.cloud.google.com/logs/query;query=resource.type=\"aiplatform.googleapis.com%2FReasoningEngine\"%0Aresource.labels.reasoning_engine_id=~\"934057118028267520\"?project=571915015654) to view the specific errors. Additionally, please check our troubleshooting guide (https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/troubleshooting/deploy) for more information, or visit our Cloud community forum (https://www.googlecloudcommunity.com/gc/AI-ML/bd-p/cloud-ai-ml) or GitHub repository (https://github.com/googleapis/python-aiplatform/issues) to find answers and ask for help.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3292e2aef108>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m remote_agent = reasoning_engines.ReasoningEngine.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     requirements=[\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"google-cloud-aiplatform[langchain,reasoningengine]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"cloudpickle==3.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/reasoning_engines/_reasoning_engines.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, reasoning_engine, requirements, reasoning_engine_name, display_name, description, gcs_dir_name, sys_version, extra_packages)\u001b[0m\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_create_with_lro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mcreated_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         _LOGGER.log_create_complete(\n\u001b[1;32m    321\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalServerError\u001b[0m: 500 \n Please navigate to the Cloud Console Log Explorer page (https://console.cloud.google.com/logs/query;query=resource.type=\"aiplatform.googleapis.com%2FReasoningEngine\"%0Aresource.labels.reasoning_engine_id=~\"934057118028267520\"?project=571915015654) to view the specific errors. Additionally, please check our troubleshooting guide (https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/troubleshooting/deploy) for more information, or visit our Cloud community forum (https://www.googlecloudcommunity.com/gc/AI-ML/bd-p/cloud-ai-ml) or GitHub repository (https://github.com/googleapis/python-aiplatform/issues) to find answers and ask for help. 13: \n Please navigate to the Cloud Console Log Explorer page (https://console.cloud.google.com/logs/query;query=resource.type=\"aiplatform.googleapis.com%2FReasoningEngine\"%0Aresource.labels.reasoning_engine_id=~\"934057118028267520\"?project=571915015654) to view the specific errors. Additionally, please check our troubleshooting guide (https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/troubleshooting/deploy) for more information, or visit our Cloud community forum (https://www.googlecloudcommunity.com/gc/AI-ML/bd-p/cloud-ai-ml) or GitHub repository (https://github.com/googleapis/python-aiplatform/issues) to find answers and ask for help."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "remote_agent.query(\n",
        "    input=\"What's the exchange rate from US dollars to Swedish currency today?\"\n",
        ")"
      ],
      "metadata": {
        "id": "J4G6AR70a87v"
      },
      "id": "J4G6AR70a87v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xkMDAXc7d1Ix"
      },
      "id": "xkMDAXc7d1Ix",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}